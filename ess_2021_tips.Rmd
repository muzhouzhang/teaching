## test

---
title: "R Tips for the 2021 Essex Summer School"
output: github_document
---
```{r options, echo = FALSE, cache = FALSE}
options(width = 200)
```

```{r load packages, warning = FALSE, error = FALSE, message = FALSE}
library(cowplot)
library(estimatr)
library(extrafont)
library(haven)
library(lubridate)
library(magrittr)
library(tidyverse)
```

```{r session info, echo = FALSE}
c(R.version.string, sessionInfo()$running)
```

## 1 Reshape
This section introduces the `tidyverse` way of reshaping rectangular data.

We use the [Apple Mobility Trends Reports](https://covid19.apple.com/mobility) to see how travel frequency changes during the Covid-19 pandemic across the UK. After loading the package, we import the CSV file using its URL and subset the data to cities in the UK only.
```{r import mobility data, message = FALSE}
uk_mobility_raw <- read_csv("https://covid19-static.cdn-apple.com/covid19-mobility-data/2114HotfixDev8/v3/en-us/applemobilitytrends-2021-08-06.csv") %>%
  filter(country == "United Kingdom", geo_type == "city") %>%
  transmute(region, transportation_type, across(c(`2020-01-13`:ncol(.))))
```

```{r show uk_mobility_raw, echo = FALSE}
uk_mobility_raw
```

### `pivot_longer`
The data is in wide format––many columns represent variable values measured at different times. But data analysis oftentimes requires long format––repeatedly measured (over time) variable values are stacked along time while a dedicated column stores the time information.

`tidyr::pivot_longer()` converts rectangular data from wide to long format. When using this function, we generally need to specify the columns that are about to be stacked.
```{r simplest pivot_longer}
uk_mobility_by_type <- uk_mobility_raw %>%
  pivot_longer(`2020-01-13`:ncol(.))
```

```{r show uk_mobility_by_type, echo = FALSE}
uk_mobility_by_type
```

We can specify the names of new columns (`name` and `value`) by using two additional arguments. They further imply how the function works: it temporally expands each row in the wide data to *T* rows in the long data and uses the column names to identify which rows correspond to which column.
```{r pivot_longer with renamed columns}
uk_mobility_by_type <- uk_mobility_raw %>%
  pivot_longer(`2020-01-13`:ncol(.), names_to = "date", values_to = "traffic")
```

We can use [selection helpers](https://dplyr.tidyverse.org/reference/select.html) to specify the columns to stack.
```{r pivot_longer with selection helpers, eval = FALSE}
uk_mobility_raw %>% pivot_longer(starts_with("20"))
uk_mobility_raw %>% pivot_longer(contains("20"))
```

### `pivot_wider`
Although the data is now stacked temporally, the unit of observation is `city`-`transportation_type`-`date`, so we want to have three separate variables (columns) for three specific types of transportation (driving, transit, and walking). `pivot_wider` does the opposite of what `pivot_longer` does.
```{r pivot_longer}
uk_mobility_panel <- uk_mobility_by_type %>%  
  pivot_wider(names_from = "transportation_type", values_from = "traffic")
```

```{r show pivot_wider, echo = FALSE}
uk_mobility_panel
```

The data finally has a typical panel structure––an *NT* × *K* matrix, in which *N* refers to cross-sectional sample size (16 cities), *T* refers to time-series sample size (572 days), and *K* refers to the number of variables, which is identical to the number of columns. Importantly, neither multiple columns represent one variable (`pivot_longer`) nor does a single column represent more than one variable (`pivot_wider`).

## 2 Date and String
This section introduces the `tidyverse` way of working with date and string. Although `uk_mobility_panel` is how typical social science panel data looks like, we use `uk_mobility_by_type` from now on for convenient Grouped Visualization (section 5).

### `lubridate`
Working with date in `numeric` or `character` type when we only have yearly data is oftentimes fine. But it is better to set date as `date` when our data's temporal frequency becomes higher. Doing so ensures time-series operations are done correctly and enables us to easily extract additional time information. We see that our `date` column's format is YYYY-MM-DD, so we use `lubridate::ymd()` to transform `date` from `character` to `date`.
```{r ymd}
# %<>% from magrittr (not recommended by many) is used to save space
uk_mobility_by_type %<>% mutate(date = ymd(date))
```

Likewise, we also have `mdy()` and `dmy()`. These functions handles both `numeric` and `character` objects and are versatile to detailed format differences, such as whether month is spelled out, leading zero is included, and so on.
```{r mdy dmy}
mdy(8102021); mdy("Jan 13 01"); dmy("1/07/1935"); dmy("1st in September in the year of 2021")
```

We may suspect that daily travel frequency is correlated with whether a day is during weekend and whether daylight saving is effective. The following two functions extract such information and create two new variables accordingly.
```{r show lubridate more}
uk_mobility_by_type %>% mutate(
  which_day = wday(date, label = TRUE),
  daylight_saving = dst(date)
  )
```

### `stringr`
With `uk_mobility_by_type` on hand, we now want to join it with UK's Covid-19 data. Specifically, we want an CSV file for *New Cases by Publish Date* in Upper Tier Local Authorities (UTLA).
```{r import covid data, message = FALSE}
uk_covid <- read_csv("https://api.coronavirus.data.gov.uk/v2/data?areaType=utla&metric=newCasesByPublishDate&format=csv&release=2021-08-11")
```

```{r show covid data, echo = FALSE}
uk_covid
```

To join data, identical row identifiers have to be in the two datasets. `uk_mobility_by_type` does not have any standardized, code-based identifier, so we have to use city names instead. However, in `uk_covid`, Bristol is named as "Bristol, City of", Edinburgh is named as "City of Edinburgh", and Glasgow is named as "Glasgow City." The code below uses `stringr::str_detect()` to modify `areaName` (to be matched to `region` in `uk_mobility_by_type` later) in `uk_covid` according to the following rule: for the observations whose `areaName` is detected to have the string `"Bristol"`, then just name their `areaName` as `"Bristol"`; for the observations whose `areaName` is detected to have the string `"Edinburgh"`, then just name their `areaName` as `"Edinburgh"`; for the observations whose `areaName` is detected to have the string `"Glasgow"`, then just name their `areaName` as `"Glasgow"`; for the observations that do not meet any of these aforementioned conditions, keep their `areaName` unchanged.
```{r change city names}
uk_covid %<>% mutate(areaName = case_when(
  str_detect(areaName, "Bristol") ~ "Bristol",
  str_detect(areaName, "Edinburgh") ~ "Edinburgh",
  str_detect(areaName, "Glasgow") ~ "Glasgow",
  TRUE ~ areaName
  ))
```

London is a single statistical unit in `uk_mobility_by_type`, but `uk_covid` provides data separately for London's 32 boroughs plus the City of London. These 33 London districts have one thing in common, though--their `areaCode` all starts with the string `"E09"`. The code below does the following: for the observations whose `areaCode` starts with the string `"E09"`, name their `areaName` as London; for the else observations, keep their `areaName` unchanged. Compared to the last chunk which changed `areaName` conditional on itself (`areaName`), this one changes `areaName` conditional on another column (`areaCode`).
```{r change london names}
uk_covid %<>% mutate(areaName = if_else(str_starts(areaCode, "E09"), "London", areaName))
```

The `stringr` package also have four style changers.
```{r show style change}
example_str <- "The `stringr` package also have four style changers"
str_to_lower(example_str); str_to_title(example_str); str_to_upper(example_str); str_to_upper(example_str) %>% str_to_sentence()
```

```{r str_to_title}
uk_mobility_by_type %<>% mutate(transportation_type = str_to_title(transportation_type))
```

## 3 Within-Group Operation
`uk_mobility_by_type` and `uk_covid` are substantively grouped (panel data). Further, `uk_mobility_by_type` is organizationally grouped too (three `transportation_type` within each `region`-`date`). This section introduces the `tidyverse` way of doing within-group operations.

### `group_by`
`uk_mobility_type` does not have data for three days. Given we only have a few missing values relative to our large temporal sample size, we decide to simply carry past values forward to impute them. 
```{r show na, echo = FALSE}
uk_mobility_by_type %>% filter(is.na(traffic))
```

But unless correctly grouping `uk_mobility_by_type` in R, we may use London's value at *t-1* for Glasgow's missing value at *t* or use Walking at *t-1* for the missing Driving at *t*. `uk_mobility_by_type` has three levels (`region`, `transportation_type`, `date`), while to fill the missing values (with `tidyr::fill()`), we only need to operate across `date`. So, we group `uk_mobility_by_type` by `region` and `transportation_type` to `fill()` within each `transportation_type` of each `city`.
```{r fill}
uk_mobility_by_type %<>%
  group_by(region, transportation_type) %>%
  fill(traffic) %>%
  ungroup()
```

The chunk below shows a section of the filled data.
```{r show fill, echo = FALSE}
uk_mobility_by_type %>% filter(date %in% c(ymd("2021-03-10"):ymd("2021-03-14")))
```

Although we can apply time-series operators during estimation (using `plm::plm()`, for example), sometimes we may want to temporally transform our variables prior to it. `dplyr` has some functions to perform basic time-series operations, but again, we need to make sure that we `group_by()` correctly.
```{r time-series operators, eval = FALSE}
uk_mobility_by_type %>%
  group_by(city, transportation_type) %>%
  mutate(
    traffic_lag = dplyr::lag(traffic),
    traffic_lag2 = dplyr::lag(traffic, 2),
    traffic_lead = dplyr::lead(traffic),
    traffic_diff = traffic - dplyr::lag(traffic)
  ) %>%
  ungroup()
```

### `group_by` and `summarize`
We can further use `dplyr::summarize()` after `group_by()` to aggregate our data, according to the aggregation method we specify, to the levels we set. In other words, we collapse all observations within the specified group to a single one. In section 2, although we renamed 33 London districts as London in `uk_covid`, London within each `date` still has 33 observations for its 33 districts. The code below adds 33 `newCasesByPublishDate` for London's 33 districts together for everyday. After that, London only has a single observation for itself as a whole on a given day, just like all other cities do.
```{r summarize london, message = FALSE}
uk_covid %<>% group_by(areaName, date) %>%
  summarize(newCasesByPublishDate = sum(newCasesByPublishDate)) %>%
  ungroup()
```

`summarize()` is useful to show unit-specific summary statistics. If we `group_by(areaName)` and then `summarize()`, we can easily have cumulative statistics of `newCasesByPublishDate` for each area. The code below lets us know the 10 UK Upper Tier Local Authorities with the most and least total Covid-19 cases.
```{r total cases}
uk_covid %>% group_by(areaName) %>%
  summarize(total_cases = sum(newCasesByPublishDate)) %>%
  arrange(desc(total_cases))

uk_covid %>% group_by(areaName) %>%
  summarize(total_cases = sum(newCasesByPublishDate)) %>%
  arrange(total_cases)
```

## 4 Join Data by Rows
This section introduces the `tidyverse` way of joining data. First, we `rename` `areaName` in `uk_covid` as `region` so that row identifiers in the two datasets have the same name, which is a necessary condition to joining data successfully.
```{r rename}
uk_covid %<>% rename(region = areaName)
```

### `left_join()`
The primary difference between various `dplyr` functions for joining data is how they deal with unmatched observations. In our example, `uk_mobility_by_type` only has 16 cities while `uk_covid` includes all Upper Tier Local Authorities in the country. Thus, the majority of observations in `uk_covid` cannot be matched. For `left_join(x, y)`, all observation in `x` (the left one) remains but all unmatched observations in `y` (the right one) are dropped. In comparison, for `left_join(y, x)`, all observations in `y` (the left one) remains but all unmatched observations in `x` (the right one) are dropped. Clearly, when applying `left_join()`, changing the relative position of the two datasets gives us opposite results. As we can see from the chunk below, when `uk_mobility_by_type` is on the left, the returned data only 16 regions since all regions in `uk_covid` but not included in `uk_mobility_by_type` are excluded from the joined data. In the opposite, all regions, no matter whether they in `uk_mobility_by_type` or not, are kept when `uk_covid` is on the left.
```{r show left_join}
left_join(uk_mobility_by_type, uk_covid, by = c("region", "date")) %>% summarize(n_distinct(region))
left_join(uk_covid, uk_mobility_by_type, by = c("region", "date")) %>% summarize(n_distinct(region))
```

### `right_join`
For `right_join(x, y)`, all observation in `y` (the right one) remains but all unmatched observations in `x` (the left one) are dropped. Obviously, the result of `right_join()` is opposite to what `left_join()` returns. We also saw from last subsection that we can also have opposite joining results by changing which data is on the left while which is on the right. Actually, `left_join(x, y)` = `right_join(y, x)` while `left_join(y, x)` = `right_join(x, y)`, holding all other arguments constant.
```{r show right_join}
right_join(uk_mobility_by_type, uk_covid, by = c("region", "date")) %>% summarize(n_distinct(region))
right_join(uk_covid, uk_mobility_by_type, by = c("region", "date")) %>% summarize(n_distinct(region))
```

### `full_join`
Neither of the four function shown above maximizes the number of observations kept in the joined data. We try to join the data on two dimensions (`region` and `date`), but in fact, the data with most regions (`uk_covid`) has fewer days. To keep as many observations as possible in the joined data, we can then use `full_join()`, which drops nothing.
```{r show full_join}
full_join(uk_mobility_by_type, uk_covid, by = c("region", "date")) %>% summarize(n_distinct(region), n_distinct(date))

uk_mobility_covid <- left_join(uk_mobility_by_type, uk_covid, by = c("region", "date")) # this is our finally joined data
```

## 5 Grouped Visualization
This section introduces the `tidyverse` way of making grouped visualization. Specifically, we want a time-series plot showing the travel frequency change (`traffic`) by three `transportation_type` by 16 cities (`region`).
```{r newcastle, include = FALSE}
# shorten the name of Newcastle upon Tyne
uk_mobility_covid %<>% mutate(region = if_else(str_detect(region, "Newcastle"), "Newcastle", region))
```

### `group and facet_wrap`
If we simply used `geom_line(aes(date, traffic))`, then we would have 48 lines wrapped together. To distinguish different `transportation_type`, we can use `transportation_type`-specific aesthetics. In the code below, three lines for each `transportation_type` are assigned to unique colors to make them mutually distinguishable (`color = transportation_type`). Alternatively, we can use `linetype = transportation_type` (straight, dotted, dashed, etc.), but given the three lines are close to each other, this is not the preferred one. It is important to note that this group setting must be inside of `aes()`. Similarly, we can apply this sort of grouping setting to other types of plots. If we do a scatter plot (`geom_point()`), for example, we can use different `shape` for different groups.

We can then use `facet_wrap()` to have a multi-panel figure, in which each panel represents the group we set. Here, we specify `vars(region)` to let panels represent different `region`. Given we have 16 cities, 4 × 4 is a natural choice regarding how many should be along rows and columns. However, we are allowed to adjust these parameters by setting `nrow` or `ncol` inside of `facet_wrap()`.
```{r facet_wrap, fig.align = 'center', fig.width = 20, fig.height = 15}
uk_mobility_covid %>% ggplot() + 
  geom_line(aes(date, traffic, group = transportation_type, color = transportation_type)) +
  facet_wrap(vars(region)) +
  scale_x_date(date_labels="%b %y") +
  labs(
    x = "", y = "", 
    title = "Daily Travel Frequency Change during the Covid-19 Pandemic in the UK", 
    subtitle = "Baseline (13 Jan 2020) = 100; Last Updated: 06 Aug 2021",
    caption = "Source: Apple Mobility Trends Reports"
  ) +
  theme_minimal_hgrid() +
  theme(
    text = element_text(family = "Palatino", size = 22),
    legend.title = element_blank(),
    strip.background = element_blank()
  )
```

### `facet_grid`
Compared to `facet_wrap()`, `facet_grid()` is able to visualize data by two group settings along two axes. In our case, it can plot by different `region` along columns and by different `transportation_type` by rows, or *vice versa*. Then, we do not need to use different aesthetics for different `transportation_type` and accordingly, there is only a single line (rather than three) within each panel.
```{r facet_grid, fig.align = 'center', fig.width = 22, fig.height = 15}
uk_mobility_covid %>% ggplot() + 
  geom_line(aes(date, traffic)) +
  facet_grid(cols = vars(region), rows = vars(transportation_type)) +
  scale_x_date(date_labels="%y", date_breaks = "1 year") +
  labs(
    x = "", y = "", 
    title = "Daily Travel Frequency Change during the Covid-19 Pandemic in the UK", 
    subtitle = "Baseline (13 Jan 2020) = 100; Last Updated: 06 Aug 2021",
    caption = "Source: Apple Mobility Trends Reports"
  ) +
  theme_bw() +
  theme(
    text = element_text(family = "Palatino", size = 22),
    legend.title = element_blank(),
    panel.grid = element_blank(),
    strip.text.x = element_text(angle = 315),
    strip.text.y = element_text(angle = 0, vjust = 0.5),
    strip.background = element_blank()
  )
```
